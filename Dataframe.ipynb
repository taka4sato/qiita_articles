{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample log.\n",
    "\n",
    "Sample log is csv with three columns, (1) date, (2) userID, (3)CampaignID  - like following.\n",
    "```\n",
    "click.at    user.id campaign.id\n",
    "2015/4/27 20:40 144012  Campaign077\n",
    "2015/4/27 0:27  24485   Campaign063\n",
    "2015/4/27 0:28  24485   Campaign063\n",
    "2015/4/27 0:33  24485   Campaign038\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[datetime.datetime(2015, 4, 27, 20, 40, 40), 144012, u'Campaign077'],\n",
       " [datetime.datetime(2015, 4, 27, 0, 27, 55), 24485, u'Campaign063'],\n",
       " [datetime.datetime(2015, 4, 27, 0, 28, 13), 24485, u'Campaign063']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, datetime, collections, commands\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "if not os.path.exists(\"./click_data_sample.csv\"):\n",
    "    print \"csv file not found at master node, will download and copy to HDFS\"\n",
    "    commands.getoutput(\"wget -q http://image.gihyo.co.jp/assets/files/book/2015/978-4-7741-7631-4/download/click_data_sample.csv\")\n",
    "    commands.getoutput(\"hadoop fs -copyFromLocal -f ./click_data_sample.csv /user/hadoop/\")\n",
    "\n",
    "whole_raw_log = sc.textFile(\"/user/hadoop/click_data_sample.csv\")\n",
    "header = whole_raw_log.first()\n",
    "whole_log = whole_raw_log.filter(lambda x:x !=header).map(lambda line: line.split(\",\"))\\\n",
    "            .map(lambda line: [datetime.datetime.strptime(line[0].replace('\"', ''), '%Y-%m-%d %H:%M:%S'), int(line[1]), line[2].replace('\"', '')])\n",
    "\n",
    "whole_log.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Dataframe\n",
    "* can create from RDD with providing schema info. [Refer this page for shema definition](http://spark.apache.org/docs/latest/sql-programming-guide.html#data-types)\n",
    "* `StructField('date', TimestampType(), True)` 's last argument (True or False) indicate \"null\" is allowed or not\n",
    "* dataframe can be cached with same manner as RDD by `dataframe.repartition(4).cache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- access_time: timestamp (nullable = true)\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- campaignID: string (nullable = true)\n",
      "\n",
      "None\n",
      "[('access_time', 'timestamp'), ('userID', 'int'), ('campaignID', 'string')]\n",
      "+--------------------+------+-----------+\n",
      "|         access_time|userID| campaignID|\n",
      "+--------------------+------+-----------+\n",
      "|2015-04-27 20:40:...|144012|Campaign077|\n",
      "|2015-04-27 00:27:...| 24485|Campaign063|\n",
      "|2015-04-27 00:28:...| 24485|Campaign063|\n",
      "|2015-04-27 00:33:...| 24485|Campaign038|\n",
      "|2015-04-27 01:00:...| 24485|Campaign063|\n",
      "|2015-04-27 16:10:...|145066|Campaign103|\n",
      "|2015-04-27 20:06:...|145066|Campaign103|\n",
      "|2015-04-27 14:52:...|167405|Campaign027|\n",
      "|2015-04-27 22:08:...|167405|Campaign027|\n",
      "|2015-04-27 20:11:...| 80524|Campaign054|\n",
      "+--------------------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "fields = [StructField('access_time', TimestampType(), True), StructField('userID', IntegerType(), True), StructField('campaignID', StringType(), True)]\n",
    "schema = StructType(fields)\n",
    "\n",
    "whole_log_df = sqlContext.createDataFrame(whole_log, schema)\n",
    "print whole_log_df.printSchema()\n",
    "print whole_log_df.dtypes\n",
    "print whole_log_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage for query with SQL expression\n",
    "* To use SQL expression, first need to set table name by `registerTempTable`\n",
    "* You can perform sub queries, but you need to add `alias` to sub query - otherwise it fails.\n",
    "  * OK : `SELECT count(*) FROM (SELECT * FROM my_table limit 10) subquery_alias`\n",
    "  * fails : `SELECT count(*) FROM (SELECT * FROM my_table limit 10)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18081\n",
      "+--------------------+------+-----------+\n",
      "|         access_time|userID| campaignID|\n",
      "+--------------------+------+-----------+\n",
      "|2015-04-27 05:26:...| 14151|Campaign047|\n",
      "|2015-04-27 05:26:...| 14151|Campaign047|\n",
      "|2015-04-27 05:26:...| 14151|Campaign047|\n",
      "|2015-04-27 05:27:...| 14151|Campaign047|\n",
      "|2015-04-27 05:28:...| 14151|Campaign047|\n",
      "+--------------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Campaign001\n",
      "+----------+\n",
      "|access_num|\n",
      "+----------+\n",
      "|      2407|\n",
      "+----------+\n",
      "\n",
      "None\n",
      "Campaign002\n",
      "+----------+\n",
      "|access_num|\n",
      "+----------+\n",
      "|      1674|\n",
      "+----------+\n",
      "\n",
      "None\n",
      "+-----------+\n",
      "|first_count|\n",
      "+-----------+\n",
      "|      20480|\n",
      "+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Simple SQL query\n",
    "\n",
    "whole_log_df.registerTempTable(\"whole_log_table\")\n",
    "\n",
    "print sqlContext.sql(\" SELECT * FROM whole_log_table where campaignID == 'Campaign047' \").count()\n",
    "print sqlContext.sql(\" SELECT * FROM whole_log_table where campaignID == 'Campaign047' \").show(5)\n",
    "\n",
    "#SQL query with variables\n",
    "for count in range(1, 3):\n",
    "    print \"Campaign00\" + str(count)\n",
    "    print sqlContext.sql(\"SELECT count(*) as access_num FROM whole_log_table where campaignID == 'Campaign00\" + str(count) + \"'\").show()\n",
    "\n",
    "#Sub-query : noted you must add alias to subquery otherwise it fails.\n",
    "print sqlContext.sql(\" SELECT count(*) as first_count FROM (SELECT userID, min(access_time) as first_access_date FROM whole_log_table GROUP BY userID) subquery_alias WHERE first_access_date < '2015-04-28'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage for `fitler` and `select`\n",
    "* `filter` extract rows with certain condition\n",
    "* `select` extract columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sample for filter\n",
    "\n",
    "print whole_log_df.filter(whole_log_df[\"access_time\"] < \"2015-04-28\").count()\n",
    "print whole_log_df.filter(whole_log_df[\"access_time\"] > \"2015-05-01\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sample for select\n",
    "\n",
    "print whole_log_df.select(\"access_time\", \"userID\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage for `groupBy + agg`\n",
    "1. example of **groupBy** which is similar of **reduceByKey**\n",
    "2. example of **groupBy + agg**\n",
    "  * argument of agg should be `agg({key:value})` : `key` to be colum name, `value` to be function name(like `min`,`sum`, `ave` etc) \n",
    "  * method of `agg` only works for the output of `groupBy`=(GroupedData Ojb), and not work for normal dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Example(1) simple groubBy -> count(). You can set multiple keys like groupBy('key1', 'key2'). \n",
    "print whole_log_df.groupBy('campaignID').count().sort('count', ascending=False).show(5)\n",
    "print whole_log_df.groupBy('campaignID', 'userID').count().sort('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Example(2)\n",
    "print whole_log_df.groupBy('userID').agg({\"access_time\": \"min\"}).show(3)\n",
    "print whole_log_df.groupBy('userID').agg({\"access_time\": \"min\"}).printSchema()\n",
    "print whole_log_df.groupBy('userID').agg({\"access_time\": \"min\"}).filter(\"min(access_time) < '2015-04-28'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF(User Defined Function)\n",
    "* Spark dataframe support UDF and typical usage will add new column (or replace existing) with UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def add_day_column(access_time):\n",
    "    return int(access_time.strftime('%Y%m%d'))\n",
    "    \n",
    "my_udf = UserDefinedFunction(add_day_column, IntegerType())\n",
    "print whole_log_df.withColumn(\"access_day\", my_udf(\"access_time\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from dataframe to another object\n",
    "* convert to RDD : `map` will convert from dataframe to RDD\n",
    "* convert to (normal) list : first convert to RDD (by `map()`), then convert to list (by `collect()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert to rdd\n",
    "print whole_log_df.groupBy('campaignID').count().map(lambda x: [x[0], x[1]]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert to list\n",
    "print whole_log_df.groupBy('campaignID').count().map(lambda x: [x[0], x[1]]).collect()[:5]\n",
    "print len(whole_log_df.groupBy('campaignID').count().map(lambda x: [x[0], x[1]]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# columns will returns list of column strings\n",
    "print whole_log_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
